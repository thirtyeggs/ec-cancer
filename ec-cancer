#!/usr/bin/env python

#--------------------------------------------------
# import modules
#--------------------------------------------------
import os.path
import sys
import argparse
import math
import re
if sys.version_info[0] < 3:
   import cPickle as pickle
else:
   import pickle

# pyvcf
# http://pyvcf.readthedocs.org/en/latest/API.html
import vcf

# pysam
# http://pysam.readthedocs.org/en/latest/api.html
import pysam



#--------------------------------------------------
# set global variables
#--------------------------------------------------
version            = "0.0"
date               = "Jul. 06, 2015"
iter_size          = 1000000
max_qs             = 80
initial_likelihood = -1
vcf_rpt_interval   = 10000
max_range          = 1000
min_cov_default    = 3
min_qs_default     = 10

if iter_size <= max_range:
   sys.exit("\nERROR: iter_size {0:%d} is smaller than max_range {1:%d}\n" % (iter_size, max_range))

# dictionary for snps
dict_normal_snp       = {}
#dict_dbsnp_snp        = {}
dict_confusion_matrix = {}
dict_complement       = {}
dict_base_to_num      = {}

dict_complement["A"] = "T"
dict_complement["C"] = "G"
dict_complement["G"] = "C"
dict_complement["T"] = "A"

dict_base_to_num["A"] = 0
dict_base_to_num["C"] = 1
dict_base_to_num["G"] = 2
dict_base_to_num["T"] = 3



#--------------------------------------------------
# parse arguments
#--------------------------------------------------
def parse_args():
   # set global variables
   global version
   global date
   global out_bam
   global out_matrix
   global out_matrix_dump
   global out_normal_vcf_dump
#   global out_dbsnp_vcf_dump
   global min_cov
   global min_cov_default
   global min_qs
   global min_qs_default

   if len(sys.argv) == 1:
      # unused options
      #print "-D  <file>       dbsnp vcf dump file       N"
      #print "-d  <file>       dbsnp vcf file            Y"

      print "USAGE: " + os.path.basename(sys.argv[0]) + " <options>"
      print ""
      print "------------------------------------------------------------------------------"
      print "ARGUMENT         DESCRIPTION               MANDATORY       DEFAULT"
      print "------------------------------------------------------------------------------"
      print "-b  <file>       tumor bam file            Y"
      print "-h               print help                N"
      print "-m  <file>       confusion matrix file     N"
      print "-n  <file>       normal bam file           Y"
      print "-q  <number>     mininum qs                N               10"
      print "-p  <string>     output prefix             Y"
      print "-t  <number>     minimum tumor coverage    N               1"
      print "-V  <file>       normal vcf dump file      Y"
      print "-v  <file>       normal vcf file           Y"
      print "------------------------------------------------------------------------------"
      print ""
      sys.exit()

   print "Parsing arguments"
   sys.stdout.flush()

   parser = argparse.ArgumentParser(description="error correction tool for normal-cancer pair alignment results")

   parser.add_argument('-b',  type=str)
   parser.add_argument('-c',  type=str)
   parser.add_argument('-D',  type=str)
   parser.add_argument('-d',  type=str)
   parser.add_argument('-m',  type=str)
   parser.add_argument('-n',  type=str)
   parser.add_argument('-q',  type=int)
   parser.add_argument('-p',  type=str)
   parser.add_argument('-r',  type=str)
   parser.add_argument('-t',  type=int)
   parser.add_argument('-V',  type=str)
   parser.add_argument('-v',  type=str)

   args = parser.parse_args()

   # check arguments
   if args.b != None:
      if args.b < 0:
         sys.exit("\nERROR: Number b cannot be < 0\n")
   else:
      sys.exit("\nERROR: Number b is not specified\n")

   if args.b == None:
      sys.exit("\nERROR: A tumor bam file is not specified\n")
   elif os.path.isfile(args.b) == False:
      sys.exit("\nERROR: " + args.b + " does not exist\n")

   if args.D != None:
      if os.path.isfile(args.D) == False:
         sys.exit("\nERROR: " + args.D + " does not exist\n")
      elif args.d != None:
         sys.exit("\nERROR: -D cannot be used with -d")

   #if args.d == None:
   #   if args.D == None:
   #      sys.exit("\nERROR: -D or -d should be used\n")
   #elif os.path.isfile(args.d) == False:
   #   sys.exit("\nERROR: " + args.d + " does not exist\n")

   if args.c == None:
      sys.exit("\nERROR: A chromosome name is not specified\n")

   if args.m != None:
      if os.path.isfile(args.m) == False:
         sys.exit("\nERROR: " + args.m + " does not exist\n")

   if args.n == None:
      sys.exit("\nERROR: A normal bam file is not specified\n")
   elif os.path.isfile(args.n) == False:
      sys.exit("\nERROR: " + args.n + " does not exist\n")

   if args.p == None:
      sys.exit("\nERROR: An output file name prefix is not specified\n")
   else:
      out_bam             = args.p + ".no-normal.bam"
      out_matrix          = args.p + ".matrix"
      out_matrix_dump     = args.p + ".matrix.dump"
      out_normal_vcf_dump = args.p + ".normal-vcf.dump"
      #out_dbsnp_vcf_dump  = args.p + ".dbsnp-vcf.dump"

   if args.r == None:
      sys.exit("\nERROR: A reference fasta file is not specified\n")
   elif os.path.isfile(args.r) == False:
      sys.exit("\nERROR: " + args.r + " does not exist\n")

   if args.t != None:
      if args.t < 1:
         sys.exit("\nERROR: Minimum tumor coverage should be >= 1\n")
      else:
         min_cov = args.t
   else:
      min_cov = min_cov_default

   if args.q != None:
      if (args.q < 1) or (args.q > 40):
         sys.exit("\nERROR: Minimum tumor coverage should be between 1-40\n")
      else:
         min_qs = args.q
   else:
      min_qs = min_qs_default

   if args.V != None:
      if os.path.isfile(args.V) == False:
         sys.exit("\nERROR: " + args.V + " does not exist\n")
      elif args.v != None:
         sys.exit("\nERROR: -V cannot be used with -v")

   if args.v == None:
      if args.V == None:
         sys.exit("\nERROR: -V or -v should be used\n")
   elif os.path.isfile(args.v) == False:
      sys.exit("\nERROR: " + args.v + " does not exist\n")

   # print arguments
   print "     Program location    : " + os.path.abspath(sys.argv[0])
   print "     Normal bam file     : " + args.n
   print "     Tumor bam file      : " + args.b

   if args.v == None:
      print "     Normal vcf dump file: " + args.V
   else:
      print "     Normal vcf file     : " + args.v

   #if args.d == None:
   #   print "     dbSNP vcf dump file : " + args.D
   #else:
   #   print "     dbSNP vcf file      : " + args.d

   print "     Reference fasta file: " + args.r
   print "     Output bam file     : " + out_bam
   print "     Chromosome          : " + args.c

   print "     Parsing arguments: done\n"
   sys.stdout.flush()

   return args



#--------------------------------------------------
# FUNCTION: is_exe
#--------------------------------------------------
def is_exe(program):
   return os.path.isfile(program) and os.access(program, os.X_OK)



#--------------------------------------------------
# FUNCTION: which
#--------------------------------------------------
def which(program):
   fpath, fname = os.path.split(program)

   if fpath:
      if is_exe(program):
         return program
   else:
      for path in os.environ['PATH'].split(os.pathsep):
         path = path.strip('"')
         exe_file = os.path.join(path, program)
         if is_exe(exe_file):
            return exe_file

   return None



#--------------------------------------------------
# FUNCTION: print_header
#--------------------------------------------------
def print_header():
   global version
   global date

   print ""
   print "------------------------------------------------------------------------------"
   print "AUTHOR : Yun Heo"
   print "VERSION: " + version
   print "DATE   : " + date
   print "------------------------------------------------------------------------------"
   print ""
   sys.stdout.flush()



#--------------------------------------------------
# FUNCTION: read_normal_vcf
#--------------------------------------------------
def read_normal_vcf():
   print "Reading a normal vcf"
   sys.stdout.flush()

   global dict_normal_snp
   global out_normal_vcf_dump

   fo_vcf = vcf.Reader(open(args.v, "r"))

   num_records = 0
   num_snps    = 0
   for each_record in fo_vcf:
      # check the chromosome name
      if each_record.CHROM == args.c:
         num_records += 1
         # check whether it is a snp
         if each_record.is_snp:
            num_snps += 1
            # each_record.CHROM was not previously added to dict_normal_snp
            if each_record.CHROM not in dict_normal_snp:
               dict_normal_snp[each_record.CHROM] = {};

            # snps in the position already exist in the dictionary
            # POS: 1-based (numbers in the vcf file)
            # ths position is converted to a 0-based number
            index = each_record.POS - 1
            # having multiple records for a locus is not allowed for a normal vcf
            if index in dict_normal_snp[each_record.CHROM]:
               sys.exit("\nERROR: CHR " + each_record.CHROM + " POS: " + each_record.POS + " previously added\n")
            else:
               dict_normal_snp[each_record.CHROM][index] = {}
               dict_normal_snp[each_record.CHROM][index][0] = each_record.alleles[0]
               dict_normal_snp[each_record.CHROM][index][1] = each_record.alleles[1:]

      # print the progress status
      if (num_records % vcf_rpt_interval) == 0:
         print "     {0:s}: {1:7d}".format(each_record.CHROM, num_records)
         sys.stdout.flush()

   # dump dict_normal_snp
   pickle.dump(dict_normal_snp, open(out_normal_vcf_dump, "wb"))

   print "     Number of records in {0:s}: {1:7d}".format(each_record.CHROM, num_records)
   print "     Number of SNPs in {0:s}   : {1:7d}".format(each_record.CHROM, num_snps)
   print "     Normal vcf dump file: " + out_normal_vcf_dump
   print "     Reading a normal vcf: done\n"
   sys.stdout.flush()



#--------------------------------------------------
# FUNCTION: load_normal_vcf
#--------------------------------------------------
def load_normal_vcf():
   print "Loading a normal vcf dump"
   sys.stdout.flush()

   global dict_normal_snp
   global out_normal_vcf_dump

   # load dict_normal_snp
   dict_normal_snp = pickle.load(open(args.V, "rb"))

   print "     Normal vcf dump file     : " + args.V
   print "     Loading a normal vcf dump: done\n"
   sys.stdout.flush()



#--------------------------------------------------
# FUNCTION: read_dbsnp_vcf
#--------------------------------------------------
def read_dbsnp_vcf():
   print "Reading a dbSNP vcf"
   sys.stdout.flush()

   global dict_dbsnp_snp
   global out_dbsnp_vcf_dump

   fo_vcf = vcf.Reader(open(args.d, "r"))

   num_records = 0
   num_snps    = 0
   for each_record in fo_vcf:
      num_records += 1
      if each_record.is_snp:
         num_snps += 1
         # each_record.CHROM was not previously added to dict_dbsnp_snp
         if each_record.CHROM not in dict_dbsnp_snp:
            dict_dbsnp_snp[each_record.CHROM] = {};

         # snps in the position already exist in the dictionary
         # POS: 1-based (numbers in the vcf file)
         # ths position is converted to a 0-based number
         index = each_record.POS - 1
         # having multiple records for a locus is not allowed for a normal vcf
         # only a last one survives
         # 22 46573683 rs17241878  G  C  .  .  CAF=[0.9169,0.0831];COMMON=1;G5;G5A;GNO;INT;KGPROD;KGPhase1;OTH;OTHERKG;RS=17241878;RSPOS=46573683;SAO=0;SLO;SSR=0;TPA;VC=SNV;VLD;VP=0x050110080011170116000100;WGT=1;dbSNPBuildID=123
         # 22 46573683 rs3052750   G  C,T   .  .  CAF=[0.9169,0.0831,.];COMMON=1;G5;GNO;INT;OTH;OTHERKG;RS=3052750;RSPOS=46573683;SAO=0;SSR=0;VC=SNV;VLD;VP=0x050000080011050102000101;WGT=1;dbSNPBuildID=113
         if index in dict_dbsnp_snp[each_record.CHROM]:
            print("     WARNING: CHR " + each_record.CHROM + " POS: " + str(each_record.POS) + " previously added")
         dict_dbsnp_snp[each_record.CHROM][index] = {}
         dict_dbsnp_snp[each_record.CHROM][index][0] = each_record.alleles[0]
         dict_dbsnp_snp[each_record.CHROM][index][1] = each_record.alleles[1:]

      # print the progress status
      if (num_records % vcf_rpt_interval) == 0:
         print "     {0:s}: {1:7d}".format(each_record.CHROM, num_records)
         sys.stdout.flush()

   # dump dict_normal_snp
   pickle.dump(dict_dbsnp_snp, open(out_dbsnp_vcf_dump, "wb"))

   print "     Number of records: {0:7d}".format(num_records)
   print "     Number of SNPs   : {0:7d}".format(num_snps)
   print "     dbSNP vcf dump file: " + out_dbsnp_vcf_dump
   print "     Reading a dbSNP vcf: done\n"
   sys.stdout.flush()



#--------------------------------------------------
# FUNCTION: load_dbsnp_vcf
#--------------------------------------------------
def load_dbsnp_vcf():
   print "Loading a dbSNP vcf dump"
   sys.stdout.flush()

   global dict_dbsnp_snp
   global out_dbsnp_vcf_dump

   # load dict_dbsnp_snp
   dict_dbsnp_snp = pickle.load(open(args.D, "rb"))

   print "     dbSNP vcf dump file     : " + args.D
   print "     Loading a dbSNP vcf dump: done\n"
   sys.stdout.flush()



#--------------------------------------------------
# FUNCTION: correct_errors
#--------------------------------------------------
def correct_errors():
   print "Correct errors"
   sys.stdout.flush()

   global out_bam
   global iter_size
   global dict_normal_snp
   global initial_likelihood
   global min_cov
   global min_qs

   fo_normal_bam = pysam.Samfile(args.b, "rb")
   fo_ref_fasta = pysam.FastaFile(args.r)
   fo_out_bam   = pysam.AlignmentFile(out_bam, "wb", template=fo_normal_bam)

   dict_header             = fo_normal_bam.header
   dict_written_reads      = {}
   dict_written_reads_prev = {}
   dict_coverage           = {}
   dict_ref_base           = {}

   list_likelihood = [-1] * 4

   # iterate chromosomes
   for each_seq in dict_header["SQ"]:
      # target chromosome
      # each_seq["SN"]: chromosome name
      # each_seq["LN"]: chromosome length
      if each_seq["SN"] == args.c:
         # read a reference fasta
         ref_seq = fo_ref_fasta.fetch(reference=each_seq["SN"], start=0, end=(each_seq["LN"] - 1))

         # set the number of iterations
         num_iters = int(math.ceil(each_seq["LN"] / float(iter_size)))

         # iterate each chunk
         # range does not include the right number
         for it_count in range(0, num_iters):
            # calculate the start/end indices
            start = it_count * iter_size

            if it_count == (num_iters - 1):
               end = each_seq["LN"] - 1;
            else:
               end = (it_count + 1) * iter_size - 1

            # fetch coverage data
            # dict_coverage is used instead of this
            # A, index 10: array_count_coverage[0][10]
            # C, index 10: array_count_coverage[1][10]
            # G, index 10: array_count_coverage[2][10]
            # T, index 10: array_count_coverage[3][10]
            #array_count_coverage = fo_normal_bam.count_coverage(each_seq["SN"], start, end, quality_threshold = 0)

            # iterate each column
            # start/end: 0-based
            for each_column in fo_normal_bam.pileup(each_seq["SN"], start, end):
               # position in the reference (0-based)
               ref_position = each_column.reference_pos
               ref_base     = ref_seq[ref_position]

               # a reference base is not N
               if ref_base != "N":
                  # initialize the dictionaries
                  dict_coverage["A"] = 0;
                  dict_coverage["C"] = 0;
                  dict_coverage["G"] = 0;
                  dict_coverage["T"] = 0;

                  dict_ref_base["A"] = 0;
                  dict_ref_base["C"] = 0;
                  dict_ref_base["G"] = 0;
                  dict_ref_base["T"] = 0;

                  # iterate each read to calculate coverage
                  for each_pileup_read in each_column.pileups:
                     # check whether is_del or is_refskip is set
                     if (not each_pileup_read.is_del) and (not each_pileup_read.is_refskip):
                        # the current base is not N
                        if each_pileup_read.alignment.query_sequence[each_pileup_read.query_position] != "N":
                           # the quality score is >= min_qs
                           if each_pileup_read.alignment.query_qualities[each_pileup_read.query_position] > min_qs:
                              dict_coverage[each_pileup_read.alignment.query_sequence[each_pileup_read.query_position]] += 1

                  # check whether this locus exists in the normal vcf file
                  if each_seq["SN"] in dict_normal_snp:
                     # a snp exist in this locus
                     if ref_position in dict_normal_snp[each_seq["SN"]]:
                        # each allele in the input vcf
                        for each_allele in dict_normal_snp[each_seq["SN"]][ref_position][1]:
                           dict_ref_base[str(each_allele)] = 1
                     # no snp
                     else:
                        dict_ref_base[ref_base] = 1

                  # iterate each read
                  for each_pileup_read in each_column.pileups:
                     # check whether is_del or is_refskip is set
                     if (not each_pileup_read.is_del) and (not each_pileup_read.is_refskip):
                        #current_base = each_pileup_read.alignment.query_sequence[each_pileup_read.query_position]
                        #current_qs   = each_pileup_read.alignment.query_qualities[each_pileup_read.query_position]
                        #e            = 10 ** (-1 * current_qs / 10.0)

                        # the current base is not N
                        if each_pileup_read.alignment.query_sequence[each_pileup_read.query_position] != "N":
                           # current base is different from the reference base
                           if dict_ref_base[each_pileup_read.alignment.query_sequence[each_pileup_read.query_position]] == 0:
                              if dict_coverage[each_pileup_read.alignment.query_sequence[each_pileup_read.query_position]] >= min_cov:
                                 if each_pileup_read.alignment.query_qualities[each_pileup_read.query_position] >= min_qs:
                                    # this read was not previously written
                                    # write it to the output file
                                    if each_pileup_read.alignment.query_name not in dict_written_reads:
                                       # it is unlikely that this read is included in dict_written_reads_prev
                                       if (ref_position - start + 1) > max_range:
                                          fo_out_bam.write(each_pileup_read.alignment)
                                          dict_written_reads[each_pileup_read.alignment.query_name] = 1
                                       else:
                                          if each_pileup_read.alignment.query_name not in dict_written_reads_prev:
                                             fo_out_bam.write(each_pileup_read.alignment)
                                             dict_written_reads[each_pileup_read.alignment.query_name] = 1

            # initialize dictionaries for written reads
            dict_written_reads_prev = dict_written_reads
            dict_written_reads      = {}

            # print the progress status
            print "     {0:s}: {1:4d} out of {2:4d}".format(each_seq["SN"], it_count + 1, num_iters)
            sys.stdout.flush()

   fo_normal_bam.close()
   fo_ref_fasta.close()
   fo_out_bam.close()

   print "     Correct errors: done\n"
   sys.stdout.flush()



#--------------------------------------------------
# FUNCTION: construct_confusion_matrix
#--------------------------------------------------
def construct_confusion_matrix():
   print "Constructing the confusion matrix"
   sys.stdout.flush()

   global dict_confusion_matrix
   global dict_normal_snp
   global dict_dbsnp_snp
   global out_matrix
   global out_matrix_dump
   global max_qs

   # initialize the confusion matrix
   initialize_confusion_matrix()

   # open files
   fo_normal_bam = pysam.Samfile(args.n, "rb")
   fo_ref_fasta = pysam.FastaFile(args.r)

   # get the header of a normal bam file
   dict_header = fo_normal_bam.header

   # iterate chromosomes
   for each_seq in dict_header["SQ"]:
      # if you want to gather information from a specific chromosome
      # add this following command
      # each_seq["SN"]: chromosome name
      # each_seq["LN"]: chromosome length
      #if each_seq["SN"] == args.c:
         # set the number of iterations
         num_iters = int(math.ceil(each_seq["LN"] / float(iter_size)))

         # fetch the corresponding sequence from the reference fasta
         ref_seq = fo_ref_fasta.fetch(reference=each_seq["SN"], start=0, end=(each_seq["LN"] - 1))

         # iterate each chunk
         # range does not include the right number
         for it_count in range(0, num_iters):
            # calculate the start/end indices
            start = it_count * iter_size

            if it_count == (num_iters - 1):
               end = each_seq["LN"] - 1;
            else:
               end = (it_count + 1) * iter_size - 1

            # fetch coverage data
            # using this if quality scores are not used
            # A, index 10: array_count_coverage[0][10]
            # C, index 10: array_count_coverage[1][10]
            # G, index 10: array_count_coverage[2][10]
            # T, index 10: array_count_coverage[3][10]
            #array_count_coverage = fo_normal_bam.count_coverage(each_seq["SN"], start, end, quality_threshold = 0)

            # iterate each column
            # start/end: 0-based
            for each_column in fo_normal_bam.pileup(each_seq["SN"], start, end):
               # position in the reference (0-based)
               ref_position = each_column.reference_pos
               ref_base     = ref_seq[ref_position]
               current_base = each_pileup_read.alignment.query_sequence[each_pileup_read.query_position]
               current_qs   = each_pileup_read.alignment.query_qualities[each_pileup_read.query_position]

               # check whether this locus is in the dbsnp vcf file
               do_this_locus = True
               if (each_seq["SN"] in dict_dbsnp_snp):
                  if (ref_position in dict_dbsnp_snp[each_seq["SN"]]):
                     do_this_locus = False

               # check whether this locus is in the normal vcf file
               if not do_this_locus:
                  if (each_seq["SN"] in dict_normal_snp):
                     if (ref_position in dict_normal_snp[each_seq["SN"]]):
                        do_this_locus = False

               # check whether a reference base is N
               if ref_base == "N":
                  do_this_locus = False

               if do_this_locus:
                  for each_pileup_read in each_column.pileups:
                     # check whether it is an indel site or a base is N
                     if (not each_pileup_read.is_del) and (not each_pileup_read.is_refskip) and (current_base != "N"):
                        # ref base : ref_base
                        # read base: current_base
                        # qs       : current_qs
                        # subtracting 33 from query_qualities is not needed
                        # but what happens for the offset 64?
                        # aligned to the reverse strand
                        if each_pileup_read.alignment.is_reverse:
                           dict_confusion_matrix[dict_complement[ref_base]][dict_complement[current_base]][current_qs] += 1
                        # aligned to the forward strand
                        else:
                           dict_confusion_matrix[ref_base][current_base][current_qs] += 1

            # print the progress status
            print "     {0:s}: {1:4d} out of {2:4d}".format(each_seq["SN"], it_count + 1, num_iters)
            sys.stdout.flush()

   fo_normal_bam.close()
   fo_ref_fasta.close()

   # write dict_confusion_matrix (readable)
   f_matrix = open(out_matrix, "w")
   for ref in ("A", "C", "G", "T"):
      for alt in ("A", "C", "G", "T"):
         for index in range(0, max_qs):
            f_matrix.write(str(dict_confusion_matrix[ref][alt][index]) + ",")
         f_matrix.write("\n")
   f_matrix.close()

   # dump dict_confusion_matrix
   pickle.dump(dict_confusion_matrix, open(out_matrix_dump, "wb"))

   print "     Output matrix file (readable): " + out_matrix
   print "     Output matrix dump file      : " + out_matrix_dump
   print "     Constructing the confusion matrix: done\n"
   sys.stdout.flush()



#--------------------------------------------------
# FUNCTION: read_confusion_matrix
#--------------------------------------------------
def read_confusion_matrix():
   print "Loading a confusion matrix"
   sys.stdout.flush()

   global dict_confusion_matrix

   # load dict_confusion_matrix
   dict_confusion_matrix = pickle.load(open(args.m, "rb"))

   print "     Matrix file: " + args.m
   print "     Loading a confusion matrix: done\n"
   sys.stdout.flush()



#--------------------------------------------------
# FUNCTION: initialize_confusion_matrix
#--------------------------------------------------
def initialize_confusion_matrix():
   global dict_confusion_matrix
   global max_qs

   # initialize the confusion matrix
   # 1st level
   dict_confusion_matrix["A"] = {}
   dict_confusion_matrix["C"] = {}
   dict_confusion_matrix["G"] = {}
   dict_confusion_matrix["T"] = {}

   # 2nd level
   dict_confusion_matrix["A"]["A"] = [0] * max_qs
   dict_confusion_matrix["A"]["C"] = [0] * max_qs
   dict_confusion_matrix["A"]["G"] = [0] * max_qs
   dict_confusion_matrix["A"]["T"] = [0] * max_qs

   dict_confusion_matrix["C"]["A"] = [0] * max_qs
   dict_confusion_matrix["C"]["C"] = [0] * max_qs
   dict_confusion_matrix["C"]["G"] = [0] * max_qs
   dict_confusion_matrix["C"]["T"] = [0] * max_qs

   dict_confusion_matrix["G"]["A"] = [0] * max_qs
   dict_confusion_matrix["G"]["C"] = [0] * max_qs
   dict_confusion_matrix["G"]["G"] = [0] * max_qs
   dict_confusion_matrix["G"]["T"] = [0] * max_qs

   dict_confusion_matrix["T"]["A"] = [0] * max_qs
   dict_confusion_matrix["T"]["C"] = [0] * max_qs
   dict_confusion_matrix["T"]["G"] = [0] * max_qs
   dict_confusion_matrix["T"]["T"] = [0] * max_qs



#--------------------------------------------------
# FUNCTION: main
#--------------------------------------------------
# print header
print_header()

# parse arguments
args = parse_args()

if args.V == None:
   # read an input vcf
   read_normal_vcf()
else:
   # load an input vcf dump
   load_normal_vcf()

#if args.D == None:
#   # read a dbsnp vcf
#   read_dbsnp_vcf()
#else:
#   # load a dbsnp vcf dump
#   load_dbsnp_vcf()
#
## read a given confusion matrix
#if args.m != None:
#   read_confusion_matrix()
## construct a confusion matrix
#else:
#   construct_confusion_matrix()

# iterate an input bam
correct_errors()

print "########################### SUCCESSFULLY COMPLETED ###########################\n"
